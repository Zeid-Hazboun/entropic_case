{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizing cases using LLM\n",
    "In this notebook, we conduct the summary of the cases using the Pegasus LLM, and examine the subsequent performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/zeidsmac/Desktop/entropic_case/data/filtered_projects.csv\") #change file path to local one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Supporting the development and success of an industrial heat pump market through the develeopment of a standardized, modular, flexible compression heat pump technology. \\n Incentive \\n High temperature industrial heat pumps are an emerging technology that play a decisive role in providing a sustainable solution for industrial heat demand up to 200°C. Previous ISPT and TKI E&I projects (CATCH-IT, STEPS) have demonstrated the operation of pilot scale compression heat pumps for industrial applications, however CAPEX costs in the order of 500 €/kWth proved to be prohibitive for market introduction.  \\n Market and cost reduction studies in the CRUISE project (ongoing) revealed that typical industrial capacities are 1-10 MWth and a lack of standardization for these units and the use of a flammable refrigerant were the most important cost determining factors. However, use of a non-flammable synthetic working medium (HFOs) still poses concerns about not being a long term sustainable solution. \\n The development and success of an industrial heat pump market is indeed dependent on the ability to produce a standardized, modular, flexible compression heat pump technology on a scale level of 1-10 MWth that uses a natural refrigerant and has low CAPEX. \\n Objectives \\n The specific goals to be achieved within the duration of the FUSE project are defined as follows: \\n Develop a full-scale (1-2 MW), steam producing, industrial heat pump using natural working media that uses waste heat sources in the range of 60°C-90°C and produces medium pressure steam (range of 2-5 bar) with a specific investment cost of < 200 €/kWth (excluding integration). Develop a modular compression heat pump design utilizing a limited number of standardized components that can be configured in numerous ways to cover > 70% of the industrial heat pump market. This enables series production and therewith low costs (< 200 €/kWth) and offers still flexibility with respect to operating conditions (power, temperature). Establish a Dutch manufacturer ( Heinen & Hopman ) for the standardized, modular, flexible compression heat pump. Establish heat pump performance under a variety of simulated (industrial process data) static and dynamical conditions at ECN part of TNO, and under field test conditions at the DOW Terneuzen site. \\n Activities \\n Within this project, a consortium of Dutch partners will cooperate to accelerate the uptake of industrial heat pumps using natural working media. A full scale (1-2 MWth) compression heat pump is designed, constructed and tested. Its performance will be characterized through testing on a specifically created industrial heat pump test infrastructure at ECN part of TNO that is able to simulate a variety of process conditions.  \\n In addition, the unit will be field tested at DOW Terneuzen. The heat pump is designed using a modular approach utilizing a limited number of standardized components that can be configured in numerous ways to cover a large share of the industrial heat pump market. Partner Heinen & Hopman will implement the necessary measures for establishing a Dutch manufacturer for industrial, steam producing heat pumps. \\n Results \\n Successful completion of this project results in an industrial compression heat pump product for a Dutch manufacturer at scale levels of 1-10 MW that can be produced in a standardized, modular, flexible, and cost effective way and uses a natural refrigerant. This product is able to serve a large share of the industrial heat pump market. As such, this project contributes to economic growth, such as the creation of new jobs in the heat pump sector and increasing the competitiveness of  Dutch industry . Finally, the uptake of industrial heat pumps contributes to the sustainability of Dutch industry through large reductions in both CO 2  emissions and primary energy consumption.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = df[\"description\"][0]\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pegasus was choosen because in an earlier assignment at university, we examined the performance of it against different models (gpt2,t5,bart) and we found that it scored exceptionally better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text):\n",
    "    pipe = pipeline(\"summarization\", model=\"google/pegasus-cnn_dailymail\")\n",
    "    pipe_out = pipe(text)\n",
    "    return pipe_out[0][\"summary_text\"].replace(\" .<n>\", \".\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Your max_length is set to 128, but your input_length is only 89. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "df[\"summary\"] = df[\"description_processed\"].apply(summarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'High temperature industrial heat pumps are an emerging technology that play a decisive role in providing a sustainable solution for industrial heat demand up to 200C.\\nPrevious ISPT and TKI E&I projects have demonstrated the operation of pilot scale compression heat pumps for industrial applications.\\nTypical industrial capacities are 1-10 MWth and a lack of standardization for these units and the use of a flammable refrigerant were the most important cost determining factors .'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"summary\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I made the summary on the preprocessed text, however it is not coherent, due to a lack of stopwords and punctuation. Therefore we regenerated the summaries based on the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "df[\"summary\"] = df[\"description\"].apply(summarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'High temperature industrial heat pumps are an emerging technology that play a decisive role in providing a sustainable solution for industrial heat demand up to 200C.\\nPrevious ISPT and TKI E&I projects have demonstrated the operation of pilot scale compression heat pumps for industrial applications.\\nTypical industrial capacities are 1-10 MWth and a lack of standardization for these units and the use of a flammable refrigerant were the most important cost determining factors .'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"summary\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of summary: 480. \n",
      "Length of original description: 3860.\n"
     ]
    }
   ],
   "source": [
    "summary_len = len(df[\"summary\"][0])\n",
    "original_len = len(df[\"description\"][0])\n",
    "print(f\"length of summary: {summary_len}. \\nLength of original description: {original_len}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating summaries for the rest of the data\n",
    "Since the data was ready, I thought I might as well create summaries for all 16 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "whole_df = pd.read_csv(\"/Users/zeidsmac/Desktop/entropic_case/data/ispt_processed.csv\") #change file path to local one\n",
    "whole_df[\"summary\"] = whole_df[\"description\"].apply(summarize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on ROUGE\n",
    "\n",
    "As an extra step, I thought we might as well evaluate how summaries did based on the ROUGE metric. The scores are quite moderate, and can be greatly improved if the model were to be finetuned on some specific data for this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.218935</td>\n",
       "      <td>0.207715</td>\n",
       "      <td>0.218935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.145560</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.395062</td>\n",
       "      <td>0.362500</td>\n",
       "      <td>0.395062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.160742</td>\n",
       "      <td>0.169492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rouge1    rouge2    rougeL\n",
       "0  0.218935  0.207715  0.218935\n",
       "1  0.153846  0.145560  0.153846\n",
       "2  0.395062  0.362500  0.395062\n",
       "3  0.169492  0.160742  0.169492"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Function to calculate ROUGE scores for pairs of text\n",
    "def calculate_rouge_scores(reference, hypothesis):\n",
    "    return scorer.score(reference, hypothesis)\n",
    "\n",
    "# Applying the ROUGE evaluation for each row in the DataFrame\n",
    "rouge_scores_list = df.apply(lambda row: calculate_rouge_scores(row['description'], row['summary']), axis=1)\n",
    "\n",
    "# Creating a new DataFrame for the ROUGE scores\n",
    "rouge_df = pd.DataFrame({\n",
    "    'rouge1': rouge_scores_list.apply(lambda x: x['rouge1'].fmeasure),\n",
    "    'rouge2': rouge_scores_list.apply(lambda x: x['rouge2'].fmeasure),\n",
    "    'rougeL': rouge_scores_list.apply(lambda x: x['rougeL'].fmeasure)\n",
    "})\n",
    "\n",
    "# Display the new DataFrame with the ROUGE scores\n",
    "rouge_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entropic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
